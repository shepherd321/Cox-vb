{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49cb1a53-b491-425d-a007-2813a03ecaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPIKE-AND-SLAB-LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b01ca8-7ed4-4fff-9e4c-2b64e3c00382",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyro-ppl torch pandas matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ae3d7-56b3-4289-8849-fe4d67265838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ba9df-0532-43d9-b2a3-29512ed8faed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc85bb4-b977-48c3-ba7e-e36e1ed06769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU Information:\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Device: NVIDIA GeForce GTX 1650\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Initial GPU Memory: 16.25 MB allocated, 42.00 MB reserved\n",
      "Loading data from: D:\\cox-model-imputation\\error-in-r-code-for-mcar\\datasets\\vb-cox\\realistic_cox_data.csv\n",
      "Data shape: (800, 1003)\n",
      "Data loading time: 0.29s\n",
      "Data loaded on: cuda:0\n",
      "Number of features: 1000\n",
      "Number of samples: 800\n",
      "Event rate: 58.88%\n",
      "\n",
      "Training fold 1/5\n",
      "\n",
      "Initialized parameters:\n",
      "  global_scale_loc: torch.Size([])\n",
      "  global_scale_scale: torch.Size([])\n",
      "  spike_prob_alpha: torch.Size([])\n",
      "  spike_prob_beta: torch.Size([])\n",
      "  indicator_probs: torch.Size([1000])\n",
      "  local_scale_loc: torch.Size([1000])\n",
      "  local_scale_scale: torch.Size([1000])\n",
      "  beta_loc: torch.Size([1000])\n",
      "  beta_scale: torch.Size([1000])\n",
      "Epoch 0: Loss = 4976091993944.2617\n",
      "Validation Loss = 4920644808012.4580\n",
      "Epoch 100: Loss = 3407639400252.8369\n",
      "Validation Loss = 3415521334237.0576\n",
      "Epoch 200: Loss = 2276986792789.6875\n",
      "Validation Loss = 2349697360057.7837\n",
      "Epoch 300: Loss = 1519212689423.5107\n",
      "Validation Loss = 1522344013248.4343\n",
      "Epoch 400: Loss = 1012535831404.4473\n",
      "Validation Loss = 981180925822.6108\n",
      "Epoch 500: Loss = 678964078756.3135\n",
      "Validation Loss = 665505594088.9744\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from pyro.optim import ClippedAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    # Training parameters\n",
    "    \"max_epochs\": 5000,\n",
    "    \"batch_size\": 128,\n",
    "    \"initial_lr\": 5e-4,\n",
    "    \"lr_decay\": 0.2,\n",
    "    \"decay_step\": 500,\n",
    "    \"clip_norm\": 10.0,\n",
    "    \"early_stop_patience\": 200,\n",
    "    \n",
    "    # Device and precision\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"precision\": torch.float32,\n",
    "    \"pin_memory\": False if torch.cuda.is_available() else True,\n",
    "    \"num_workers\": 0,\n",
    "    \"cuda_deterministic\": False,\n",
    "    \n",
    "    # Model parameters\n",
    "    \"elbo_particles\": 10,\n",
    "    \"warmup_epochs\": 100,\n",
    "    \"n_folds\": 5,\n",
    "    \n",
    "    # Spike-and-slab lasso parameters\n",
    "    \"max_risk_score\": 50.0,\n",
    "    \"min_scale\": 1e-10,\n",
    "    \"init_spike_prob\": 0.5,\n",
    "    \"slab_df\": 1.0,\n",
    "    \"lasso_scale\": 1.0,\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    \"log_freq\": 100,\n",
    "    \"checkpoint_freq\": 500,\n",
    "    \n",
    "    # Paths\n",
    "    \"data_path\": r\"D:\\cox-model-imputation\\error-in-r-code-for-mcar\\datasets\\vb-cox\\realistic_cox_data.csv\",\n",
    "    \"results_dir\": f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "}\n",
    "\n",
    "class CoxPartialLikelihood(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "        self.max_exp = 20.0\n",
    "        \n",
    "    def forward(self, risk_scores, survival_time, event):\n",
    "        event_mask = event == 1\n",
    "        n_events = event_mask.sum()\n",
    "        \n",
    "        if n_events < 1:\n",
    "            return torch.tensor(0.0, device=risk_scores.device)\n",
    "        \n",
    "        # Numerical stability: normalize risk scores\n",
    "        max_risk = risk_scores.max()\n",
    "        risk_scores = risk_scores - max_risk\n",
    "        exp_risk = torch.exp(torch.clamp(risk_scores, min=-self.max_exp))\n",
    "        \n",
    "        # Compute risk matrix with stable operations\n",
    "        risk_matrix = (survival_time.unsqueeze(0) >= \n",
    "                      survival_time[event_mask].unsqueeze(1)).float()\n",
    "        \n",
    "        # Compute cumulative hazard with numerical stability\n",
    "        cumsum_exp_risk = torch.clamp(\n",
    "            (risk_matrix * exp_risk.unsqueeze(0)).sum(dim=1),\n",
    "            min=self.eps\n",
    "        )\n",
    "        \n",
    "        log_sum_exp = torch.log(cumsum_exp_risk)\n",
    "        \n",
    "        # Compute partial likelihood\n",
    "        events_risk_score = risk_scores[event_mask]\n",
    "        partial_likelihood = (events_risk_score - log_sum_exp).sum()\n",
    "        \n",
    "        # Normalize by batch size for better scaling\n",
    "        return partial_likelihood / risk_scores.size(0)\n",
    "\n",
    "def model(X, survival_time, event):\n",
    "    n, p = X.shape\n",
    "    device = X.device\n",
    "    \n",
    "    # Global shrinkage parameter (for the slab component)\n",
    "    global_scale = pyro.sample(\"global_scale\", \n",
    "                             dist.HalfCauchy(torch.tensor(1.0, device=device)))\n",
    "    \n",
    "    # Spike probability\n",
    "    spike_prob = pyro.sample(\"spike_prob\",\n",
    "                            dist.Beta(torch.tensor(1.0, device=device),\n",
    "                                    torch.tensor(1.0, device=device)))\n",
    "    \n",
    "    with pyro.plate(\"features\", p, dim=-1):\n",
    "        # Binary indicators for spike-and-slab\n",
    "        indicators = pyro.sample(\"indicators\",\n",
    "                               dist.Bernoulli(probs=spike_prob))\n",
    "        \n",
    "        # Local scale parameters (for the slab component)\n",
    "        local_scale = pyro.sample(\"local_scale\", \n",
    "                                dist.HalfCauchy(torch.tensor(1.0, device=device)))\n",
    "        \n",
    "        # Combine spike and slab components with numerical stability\n",
    "        scale = (indicators * global_scale * local_scale + \n",
    "                (1 - indicators) * torch.tensor(1e-10, device=device))\n",
    "        \n",
    "        # Sample coefficients\n",
    "        beta = pyro.sample(\"beta\", \n",
    "                          dist.Laplace(torch.zeros(p, device=device), scale))\n",
    "    \n",
    "    # Compute risk scores with numerical stability\n",
    "    risk_scores = torch.clamp(X @ beta, min=-50.0, max=50.0)\n",
    "    \n",
    "    # Compute likelihood with stable implementation\n",
    "    cox_likelihood = CoxPartialLikelihood()\n",
    "    log_pl = cox_likelihood(risk_scores, survival_time, event)\n",
    "    pyro.factor(\"log_pl\", log_pl)\n",
    "    \n",
    "    return beta\n",
    "\n",
    "def guide(X, survival_time, event):\n",
    "    n, p = X.shape\n",
    "    device = X.device\n",
    "    \n",
    "    # Global scale parameters\n",
    "    global_scale_loc = pyro.param(\n",
    "        \"global_scale_loc\",\n",
    "        torch.tensor(1.0, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    global_scale_scale = pyro.param(\n",
    "        \"global_scale_scale\",\n",
    "        torch.tensor(0.1, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    \n",
    "    # Spike probability parameters\n",
    "    spike_prob_alpha = pyro.param(\n",
    "        \"spike_prob_alpha\",\n",
    "        torch.tensor(1.0, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    spike_prob_beta = pyro.param(\n",
    "        \"spike_prob_beta\",\n",
    "        torch.tensor(1.0, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    \n",
    "    # Sample global scale\n",
    "    pyro.sample(\n",
    "        \"global_scale\",\n",
    "        dist.LogNormal(global_scale_loc.log(), global_scale_scale)\n",
    "    )\n",
    "    \n",
    "    # Sample spike probability\n",
    "    pyro.sample(\n",
    "        \"spike_prob\",\n",
    "        dist.Beta(spike_prob_alpha, spike_prob_beta)\n",
    "    )\n",
    "    \n",
    "    with pyro.plate(\"features\", p, dim=-1):\n",
    "        # Indicator parameters\n",
    "        indicator_probs = pyro.param(\n",
    "            \"indicator_probs\",\n",
    "            torch.ones(p, device=device) * config[\"init_spike_prob\"],\n",
    "            constraint=constraints.interval(0.0, 1.0)\n",
    "        )\n",
    "        \n",
    "        # Local scale parameters\n",
    "        local_scale_loc = pyro.param(\n",
    "            \"local_scale_loc\",\n",
    "            torch.ones(p, device=device),\n",
    "            constraint=constraints.positive\n",
    "        )\n",
    "        local_scale_scale = pyro.param(\n",
    "            \"local_scale_scale\",\n",
    "            torch.ones(p, device=device) * 0.1,\n",
    "            constraint=constraints.positive\n",
    "        )\n",
    "        \n",
    "        # Beta parameters\n",
    "        beta_loc = pyro.param(\n",
    "            \"beta_loc\",\n",
    "            torch.zeros(p, device=device)\n",
    "        )\n",
    "        beta_scale = pyro.param(\n",
    "            \"beta_scale\",\n",
    "            torch.ones(p, device=device) * config[\"lasso_scale\"],\n",
    "            constraint=constraints.positive\n",
    "        )\n",
    "        \n",
    "        # Sample indicators\n",
    "        pyro.sample(\"indicators\", dist.Bernoulli(indicator_probs))\n",
    "        \n",
    "        # Sample local scales\n",
    "        pyro.sample(\n",
    "            \"local_scale\",\n",
    "            dist.LogNormal(local_scale_loc.log(), local_scale_scale)\n",
    "        )\n",
    "        \n",
    "        # Sample beta with Laplace distribution\n",
    "        pyro.sample(\"beta\", dist.Laplace(beta_loc, beta_scale))\n",
    "\n",
    "def initialize_params():\n",
    "    # This function ensures all parameters are registered before training\n",
    "    device = config[\"device\"]\n",
    "    \n",
    "    # Initialize with dummy tensors just to register parameters\n",
    "    X_dummy = torch.zeros((10, 1000), device=device)\n",
    "    time_dummy = torch.zeros(10, device=device)\n",
    "    event_dummy = torch.zeros(10, device=device)\n",
    "    \n",
    "    # Run model and guide once with dummy data to register all parameters\n",
    "    pyro.clear_param_store()\n",
    "    try:\n",
    "        model(X_dummy, time_dummy, event_dummy)\n",
    "        guide(X_dummy, time_dummy, event_dummy)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing parameters: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Print registered parameters\n",
    "    print(\"\\nInitialized parameters:\")\n",
    "    for name, param in pyro.get_param_store().items():\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "#=========================\n",
    "def setup_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "        print(\"\\nGPU Information:\")\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Initial GPU Memory: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB allocated, {torch.cuda.memory_reserved(0)/1024**2:.2f} MB reserved\")\n",
    "        \n",
    "        torch.backends.cudnn.deterministic = config[\"cuda_deterministic\"]\n",
    "        torch.backends.cudnn.benchmark = not config[\"cuda_deterministic\"]\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nNo GPU available, using CPU\")\n",
    "        return False\n",
    "\n",
    "def load_and_preprocess(path, config):\n",
    "    print(f\"Loading data from: {path}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(path)\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "    \n",
    "    X = data.iloc[:, 1:-2].values\n",
    "    time_values = data['time'].values\n",
    "    event_values = data['event'].values\n",
    "    \n",
    "    X = (X - np.nanmean(X, axis=0)) / (np.nanstd(X, axis=0) + 1e-8)\n",
    "    X = np.nan_to_num(X)\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    time_tensor = torch.tensor(time_values, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    event_tensor = torch.tensor(event_values, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    \n",
    "    print(f\"Data loading time: {time.time()-start_time:.2f}s\")\n",
    "    print(f\"Data loaded on: {X_tensor.device}\")\n",
    "    print(f\"Number of features: {X_tensor.shape[1]}\")\n",
    "    print(f\"Number of samples: {X_tensor.shape[0]}\")\n",
    "    print(f\"Event rate: {event_tensor.mean().item():.2%}\")\n",
    "    \n",
    "    return X_tensor, time_tensor, event_tensor\n",
    "\n",
    "class BayesianCoxModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config[\"device\"]\n",
    "        pyro.clear_param_store()\n",
    "        initialize_params()\n",
    "\n",
    "    def train(self, train_loader, val_loader=None):\n",
    "        optimizer = ClippedAdam({\n",
    "            \"lr\": self.config[\"initial_lr\"],\n",
    "            \"clip_norm\": self.config[\"clip_norm\"],\n",
    "            \"weight_decay\": 1e-4\n",
    "        })\n",
    "        \n",
    "        svi = SVI(model, guide, optimizer, \n",
    "                 loss=Trace_ELBO(num_particles=self.config[\"elbo_particles\"]))\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        patience = 0\n",
    "        param_trajectories = {}\n",
    "        \n",
    "        for name in pyro.get_param_store().keys():\n",
    "            param_trajectories[name] = []\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(self.config[\"max_epochs\"]):\n",
    "                epoch_loss = 0.0\n",
    "                for batch_idx, (X_batch, t_batch, e_batch) in enumerate(train_loader):\n",
    "                    X_batch = X_batch.to(self.device)\n",
    "                    t_batch = t_batch.to(self.device)\n",
    "                    e_batch = e_batch.to(self.device)\n",
    "                    \n",
    "                    loss = svi.step(X_batch, t_batch, e_batch)\n",
    "                    epoch_loss += loss\n",
    "                \n",
    "                avg_loss = epoch_loss / len(train_loader)\n",
    "                losses.append(avg_loss)\n",
    "                \n",
    "                # Store parameter trajectories\n",
    "                for name, param in pyro.get_param_store().items():\n",
    "                    param_trajectories[name].append(param.detach().cpu().numpy())\n",
    "                \n",
    "                if val_loader is not None:\n",
    "                    val_loss = self.evaluate(svi, val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        patience = 0\n",
    "                        self.save_checkpoint(epoch, val_loss)\n",
    "                    else:\n",
    "                        patience += 1\n",
    "                    \n",
    "                    if patience >= self.config[\"early_stop_patience\"]:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                \n",
    "                if epoch % self.config[\"decay_step\"] == 0 and epoch > 0:\n",
    "                    current_lr = self.config[\"initial_lr\"] * \\\n",
    "                               (self.config[\"lr_decay\"] ** (epoch // self.config[\"decay_step\"]))\n",
    "                    optimizer.set_state({'lr': current_lr})\n",
    "                    \n",
    "                if epoch % self.config[\"log_freq\"] == 0:\n",
    "                    print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
    "                    if val_loader is not None:\n",
    "                        print(f\"Validation Loss = {val_losses[-1]:.4f}\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {e}\")\n",
    "            raise\n",
    "        \n",
    "        return losses, val_losses, param_trajectories\n",
    "    \n",
    "    def evaluate(self, svi, loader):\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, t_batch, e_batch in loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                t_batch = t_batch.to(self.device)\n",
    "                e_batch = e_batch.to(self.device)\n",
    "                val_loss += svi.evaluate_loss(X_batch, t_batch, e_batch)\n",
    "        return val_loss / len(loader)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_loss):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'val_loss': val_loss,\n",
    "            'param_store': pyro.get_param_store().get_state()\n",
    "        }\n",
    "        torch.save(checkpoint, \n",
    "                  os.path.join(self.config[\"results_dir\"], f'best_model.pth'))\n",
    "\n",
    "def cross_validate(model_class, X_tensor, time_tensor, event_tensor, config):\n",
    "    device = config[\"device\"]\n",
    "    kf = KFold(n_splits=config[\"n_folds\"], shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    \n",
    "    X_np = X_tensor.cpu().numpy()\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_np)):\n",
    "        print(f\"\\nTraining fold {fold + 1}/{config['n_folds']}\")\n",
    "        \n",
    "        train_idx = torch.tensor(train_idx, device=device)\n",
    "        val_idx = torch.tensor(val_idx, device=device)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            TensorDataset(X_tensor[train_idx], time_tensor[train_idx], event_tensor[train_idx]),\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            pin_memory=False,\n",
    "            generator=torch.Generator(device=device)\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            TensorDataset(X_tensor[val_idx], time_tensor[val_idx], event_tensor[val_idx]),\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            pin_memory=False,\n",
    "            generator=torch.Generator(device=device)\n",
    "        )\n",
    "        \n",
    "        model = model_class(config)\n",
    "        losses, val_losses, param_trajectories = model.train(train_loader, val_loader)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            beta = pyro.param(\"beta_loc\").detach()\n",
    "            indicators = pyro.param(\"indicator_probs\").detach()\n",
    "            risk_scores = X_tensor[val_idx] @ beta\n",
    "            \n",
    "            metrics = {\n",
    "                'c_index': concordance_index(\n",
    "                    time_tensor[val_idx].cpu().numpy(),\n",
    "                    -risk_scores.cpu().numpy(),\n",
    "                    event_tensor[val_idx].cpu().numpy()\n",
    "                ),\n",
    "                'final_loss': float(losses[-1]),\n",
    "                'param_trajectories': param_trajectories,\n",
    "                'val_losses': val_losses,\n",
    "                'last_epoch': len(losses),\n",
    "                'selected_features': (indicators > 0.5).float().cpu().numpy()\n",
    "            }\n",
    "            \n",
    "            fold_results.append(metrics)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "def plot_diagnostics(results_dir, fold_results, losses, val_losses_list, param_trajectories):\n",
    "    plots_dir = os.path.join(results_dir, 'plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(losses, label='Training Loss', color='blue', linewidth=2)\n",
    "    \n",
    "    if val_losses_list:\n",
    "        for fold_idx, val_losses in enumerate(val_losses_list):\n",
    "            plt.plot(val_losses, label=f'Validation Loss (Fold {fold_idx+1})', \n",
    "                     linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title('Training and Validation Loss Convergence', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('ELBO Loss', fontsize=12)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'loss_convergence.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot feature selection probabilities\n",
    "    if 'indicator_probs' in param_trajectories:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        final_probs = param_trajectories['indicator_probs'][-1]\n",
    "        plt.hist(final_probs, bins=50, alpha=0.7)\n",
    "        plt.title('Feature Selection Probabilities Distribution', fontsize=14)\n",
    "        plt.xlabel('Selection Probability', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'feature_selection_dist.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot selected features heatmap\n",
    "    selected_features = np.stack([result['selected_features'] for result in fold_results])\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(selected_features, cmap='coolwarm', center=0.5,\n",
    "                xticklabels=False, yticklabels=range(1, len(fold_results) + 1))\n",
    "    plt.title('Selected Features Across Folds', fontsize=14)\n",
    "    plt.xlabel('Feature Index', fontsize=12)\n",
    "    plt.ylabel('Fold', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'selected_features.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot C-index distribution\n",
    "    c_indices = [result['c_index'] for result in fold_results]\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.violinplot(c_indices, showmeans=True, showmedians=True)\n",
    "    plt.plot([1] * len(c_indices), c_indices, 'o', color='blue', alpha=0.7)\n",
    "    mean_c = np.mean(c_indices)\n",
    "    std_c = np.std(c_indices)\n",
    "    plt.text(1.2, min(c_indices), \n",
    "             f'Mean: {mean_c:.4f}\\nStd: {std_c:.4f}', \n",
    "             fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n",
    "    plt.title('Cross-validation C-index Distribution', fontsize=14)\n",
    "    plt.ylabel('C-index', fontsize=12)\n",
    "    plt.xticks([1], ['C-index'])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'cv_c_index.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    setup_gpu()\n",
    "    os.makedirs(config[\"results_dir\"], exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(config[\"results_dir\"], 'config.json'), 'w') as f:\n",
    "        json.dump({k: str(v) if isinstance(v, (torch.dtype, type)) else v \n",
    "                  for k, v in config.items()}, f, indent=2)\n",
    "    \n",
    "    try:\n",
    "        X_tensor, time_tensor, event_tensor = load_and_preprocess(config[\"data_path\"], config)\n",
    "        fold_results = cross_validate(BayesianCoxModel, X_tensor, time_tensor, event_tensor, config)\n",
    "        \n",
    "        all_val_losses = [result['val_losses'] for result in fold_results if 'val_losses' in result]\n",
    "        cv_stopping_epochs = [result['last_epoch'] for result in fold_results if 'last_epoch' in result]\n",
    "        \n",
    "        optimal_epochs = int(np.mean(cv_stopping_epochs)) if cv_stopping_epochs else 1700\n",
    "        print(f\"\\nUsing {optimal_epochs} epochs for final model based on cross-validation\")\n",
    "        \n",
    "        final_config = config.copy()\n",
    "        final_config[\"max_epochs\"] = optimal_epochs\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, time_tensor, event_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            pin_memory=False,\n",
    "            generator=torch.Generator(device=config[\"device\"])\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTraining final model on full dataset for {optimal_epochs} epochs\")\n",
    "        final_model = BayesianCoxModel(final_config)\n",
    "        losses, _, param_trajectories = final_model.train(train_loader)\n",
    "        \n",
    "        plot_diagnostics(config[\"results_dir\"], fold_results, losses, all_val_losses, param_trajectories)\n",
    "        \n",
    "        results_summary = {\n",
    "            'mean_c_index': float(np.mean([r['c_index'] for r in fold_results])),\n",
    "            'std_c_index': float(np.std([r['c_index'] for r in fold_results])),\n",
    "            'final_loss': float(losses[-1]),\n",
    "            'n_epochs_final': len(losses),\n",
    "            'optimal_epochs': optimal_epochs,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(config[\"results_dir\"], 'results_summary.json'), 'w') as f:\n",
    "            json.dump(results_summary, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\nTraining completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        print(\"\\nExecution finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd424dc4-f394-41c9-9e03-40f8bcb1c644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0247711-44db-4046-8e9b-193075bfeafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92e86c4-fc1c-48ee-b387-e433a7d661a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHIKOM~1\\ANACON~1\\envs\\vb_cox_pymc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA version: 12.1\n",
      "Current device: 0\n",
      "Current Date and Time (UTC - YYYY-MM-DD HH:MM:SS formatted): 2025-03-10 23:29:37\n",
      "Current User's Login: CHIKOMANA\n",
      "\n",
      "Loading data from: D:\\cox-model-imputation\\error-in-r-code-for-mcar\\datasets\\vb-cox\\realistic_cox_data.csv\n",
      "Data shape: (800, 1003)\n",
      "Number of features: 1000\n",
      "Number of samples: 800\n",
      "Event rate: 58.88%\n",
      "\n",
      "Training fold 1/5\n",
      "Epoch    0 | Train Loss: 442121988.98 | Val Loss: 433382492.14\n",
      "Epoch   50 | Train Loss: 51391597.47 | Val Loss: 49756325.93\n",
      "Epoch  100 | Train Loss: 5228408.21 | Val Loss: 5122794.27\n",
      "Epoch  150 | Train Loss: 595467.10 | Val Loss: 585467.14\n",
      "Epoch  200 | Train Loss: 121392.58 | Val Loss: 123495.56\n",
      "Epoch  250 | Train Loss: 45958.49 | Val Loss: 44624.07\n",
      "Epoch  300 | Train Loss: 27288.88 | Val Loss: 27833.89\n",
      "Epoch  350 | Train Loss: 22884.63 | Val Loss: 21726.77\n",
      "Epoch  400 | Train Loss: 21106.68 | Val Loss: 22002.78\n",
      "Epoch  450 | Train Loss: 21732.67 | Val Loss: 22513.28\n",
      "\n",
      "Early stopping at epoch 469\n",
      "Fold 1 completed - C-index: 0.533\n",
      "\n",
      "Training fold 2/5\n",
      "Epoch    0 | Train Loss: 444642071.14 | Val Loss: 434865151.42\n",
      "Epoch   50 | Train Loss: 51567283.44 | Val Loss: 49642836.56\n",
      "Epoch  100 | Train Loss: 5240012.21 | Val Loss: 5151800.00\n",
      "Epoch  150 | Train Loss: 603119.96 | Val Loss: 583631.28\n",
      "Epoch  200 | Train Loss: 120364.36 | Val Loss: 118993.74\n",
      "Epoch  250 | Train Loss: 46359.82 | Val Loss: 47856.55\n",
      "Epoch  300 | Train Loss: 27312.90 | Val Loss: 27047.27\n",
      "Epoch  350 | Train Loss: 23181.93 | Val Loss: 23952.25\n",
      "Epoch  400 | Train Loss: 21680.67 | Val Loss: 21534.70\n",
      "\n",
      "Early stopping at epoch 430\n",
      "Fold 2 completed - C-index: 0.478\n",
      "\n",
      "Training fold 3/5\n",
      "Epoch    0 | Train Loss: 442455562.88 | Val Loss: 435751727.07\n",
      "Epoch   50 | Train Loss: 51860622.91 | Val Loss: 49631300.28\n",
      "Epoch  100 | Train Loss: 5293633.07 | Val Loss: 5148002.94\n",
      "Epoch  150 | Train Loss: 606654.68 | Val Loss: 591243.53\n",
      "Epoch  200 | Train Loss: 122670.24 | Val Loss: 117967.20\n",
      "Epoch  250 | Train Loss: 46190.79 | Val Loss: 45296.94\n",
      "Epoch  300 | Train Loss: 28733.27 | Val Loss: 29492.33\n",
      "Epoch  350 | Train Loss: 22529.42 | Val Loss: 22919.50\n",
      "Epoch  400 | Train Loss: 21802.38 | Val Loss: 21917.77\n",
      "Epoch  450 | Train Loss: 21632.06 | Val Loss: 21814.07\n",
      "\n",
      "Early stopping at epoch 478\n",
      "Fold 3 completed - C-index: 0.493\n",
      "\n",
      "Training fold 4/5\n",
      "Epoch    0 | Train Loss: 442053492.58 | Val Loss: 431766191.79\n",
      "Epoch   50 | Train Loss: 51612125.55 | Val Loss: 49961844.90\n",
      "Epoch  100 | Train Loss: 5198068.95 | Val Loss: 5169418.95\n",
      "Epoch  150 | Train Loss: 596502.01 | Val Loss: 565811.61\n",
      "Epoch  200 | Train Loss: 122028.71 | Val Loss: 118976.88\n",
      "Epoch  250 | Train Loss: 46109.12 | Val Loss: 46350.42\n",
      "Epoch  300 | Train Loss: 28100.18 | Val Loss: 26211.69\n",
      "Epoch  350 | Train Loss: 22686.40 | Val Loss: 22196.02\n",
      "Epoch  400 | Train Loss: 21889.73 | Val Loss: 21406.16\n",
      "Epoch  450 | Train Loss: 21853.26 | Val Loss: 22273.64\n",
      "\n",
      "Early stopping at epoch 482\n",
      "Fold 4 completed - C-index: 0.509\n",
      "\n",
      "Training fold 5/5\n",
      "Epoch    0 | Train Loss: 441162461.27 | Val Loss: 428566084.50\n",
      "Epoch   50 | Train Loss: 51589325.76 | Val Loss: 50409250.02\n",
      "Epoch  100 | Train Loss: 5263328.86 | Val Loss: 5097054.99\n",
      "Epoch  150 | Train Loss: 601583.87 | Val Loss: 601700.43\n",
      "Epoch  200 | Train Loss: 122050.45 | Val Loss: 116293.57\n",
      "Epoch  250 | Train Loss: 47050.74 | Val Loss: 43825.12\n",
      "Epoch  300 | Train Loss: 27302.58 | Val Loss: 29375.43\n",
      "Epoch  350 | Train Loss: 22993.52 | Val Loss: 23204.57\n",
      "Epoch  400 | Train Loss: 21402.66 | Val Loss: 21161.70\n",
      "Epoch  450 | Train Loss: 21262.81 | Val Loss: 21159.45\n",
      "\n",
      "Early stopping at epoch 452\n",
      "Fold 5 completed - C-index: 0.496\n",
      "\n",
      "Using 463 epochs for final model based on cross-validation\n",
      "\n",
      "Training final model on full dataset for 463 epochs\n",
      "Epoch    0 | Train Loss: 436965222.96\n",
      "Epoch   50 | Train Loss: 29208980.39\n",
      "Epoch  100 | Train Loss: 1698599.65\n",
      "Epoch  150 | Train Loss: 170103.65\n",
      "Epoch  200 | Train Loss: 46741.73\n",
      "Epoch  250 | Train Loss: 25811.83\n",
      "Epoch  300 | Train Loss: 21564.08\n",
      "Epoch  350 | Train Loss: 21362.31\n",
      "Epoch  400 | Train Loss: 22530.18\n",
      "Epoch  450 | Train Loss: 25025.92\n",
      "Epoch  500 | Train Loss: 28230.26\n",
      "Epoch  550 | Train Loss: 31519.55\n",
      "Epoch  600 | Train Loss: 36250.73\n",
      "Epoch  650 | Train Loss: 41138.92\n",
      "Epoch  700 | Train Loss: 49598.38\n",
      "Epoch  750 | Train Loss: 57444.59\n",
      "\n",
      "Training completed\n",
      "\n",
      "Execution finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from pyro.optim import ClippedAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter, CoxPHFitter\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Configuration\n",
    "# config = {\n",
    "#     # Training parameters\n",
    "#     \"max_epochs\": 800,\n",
    "#     \"batch_size\": 32,  # Reduced batch size for better stability\n",
    "#     \"initial_lr\": 1e-3,  # Increased learning rate for faster initial convergence\n",
    "#     \"lr_decay\": 0.5,   # More aggressive decay for better fine-tuning\n",
    "#     \"decay_step\": 100,  # More frequent decay steps\n",
    "#     \"clip_norm\": 0.1,  # Reduced clip norm for better gradient stability\n",
    "#     \"early_stop_patience\": 50,\n",
    "    \n",
    "#     # Device\n",
    "#     \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     \"precision\": torch.float32,\n",
    "    \n",
    "#     # Model parameters\n",
    "#     \"elbo_particles\": 5,\n",
    "#     \"n_folds\": 5,\n",
    "    \n",
    "#     # Regularization parameters\n",
    "#     \"max_risk_score\": 3.0,  # Reduced max risk score for better numerical stability\n",
    "#     \"min_scale\": 1e-4,  # Increased min scale to prevent underflow\n",
    "#     \"init_spike_prob\": 0.1,  # Increased initial probability for better feature selection\n",
    "#     \"global_scale_prior\": 0.1,  # Increased prior scale for better initialization\n",
    "#     \"lasso_scale\": 0.1,  # Increased lasso scale for smoother optimization\n",
    "    \n",
    "#     # Paths\n",
    "#     \"data_path\": r\"D:\\cox-model-imputation\\error-in-r-code-for-mcar\\datasets\\vb-cox\\realistic_cox_data.csv\",\n",
    "#     \"results_dir\": f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "# }\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    # Training parameters\n",
    "    \"max_epochs\": 800,\n",
    "    \"batch_size\": 32,\n",
    "    \"initial_lr\": 1e-3,\n",
    "    \"lr_decay\": 0.5,\n",
    "    \"decay_step\": 100,\n",
    "    \"clip_norm\": 0.1,\n",
    "    \"early_stop_patience\": 50,\n",
    "    \"log_freq\": 50,  # Add logging frequency parameter\n",
    "    \n",
    "    # Device\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"precision\": torch.float32,\n",
    "    \n",
    "    # Model parameters\n",
    "    \"elbo_particles\": 5,\n",
    "    \"n_folds\": 5,\n",
    "    \n",
    "    # Regularization parameters\n",
    "    \"max_risk_score\": 3.0,\n",
    "    \"min_scale\": 1e-4,\n",
    "    \"init_spike_prob\": 0.1,\n",
    "    \"global_scale_prior\": 0.1,\n",
    "    \"lasso_scale\": 0.1,\n",
    "    \n",
    "    # Paths\n",
    "    \"data_path\": r\"D:\\cox-model-imputation\\error-in-r-code-for-mcar\\datasets\\vb-cox\\realistic_cox_data.csv\",\n",
    "    \"results_dir\": f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "}\n",
    "\n",
    "class CoxPartialLikelihood(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "        self.max_exp = 20.0\n",
    "        \n",
    "    def forward(self, risk_scores, survival_time, event):\n",
    "        event_mask = event == 1\n",
    "        n_events = event_mask.sum()\n",
    "        \n",
    "        if n_events < 1:\n",
    "            return torch.tensor(0.0, device=risk_scores.device)\n",
    "        \n",
    "        risk_scores = risk_scores - risk_scores.mean()\n",
    "        risk_scores = torch.clamp(risk_scores, min=-self.max_exp, max=self.max_exp)\n",
    "        \n",
    "        sorted_times, indices = torch.sort(survival_time, descending=True)\n",
    "        risk_scores = risk_scores[indices]\n",
    "        event_mask = event_mask[indices]\n",
    "        \n",
    "        risk_scores_exp = torch.exp(risk_scores)\n",
    "        cumsum_risk = torch.cumsum(risk_scores_exp, dim=0)\n",
    "        log_cumsum_risk = torch.log(cumsum_risk + self.eps)\n",
    "        \n",
    "        event_likelihood = risk_scores[event_mask] - log_cumsum_risk[event_mask]\n",
    "        \n",
    "        # Remove the division by n_events\n",
    "        return event_likelihood.sum()\n",
    "\n",
    "def model(X, survival_time, event):\n",
    "    n, p = X.shape\n",
    "    device = X.device\n",
    "    \n",
    "    # More conservative scale for global shrinkage\n",
    "    global_scale = pyro.sample(\"global_scale\", \n",
    "                             dist.HalfCauchy(torch.tensor(0.1, device=device)))\n",
    "    \n",
    "    # More informative prior for spike probability\n",
    "    spike_prob = pyro.sample(\"spike_prob\",\n",
    "                            dist.Beta(torch.tensor(2.0, device=device),\n",
    "                                    torch.tensor(2.0, device=device)))\n",
    "    \n",
    "    with pyro.plate(\"features\", p, dim=-1):\n",
    "        # Binary indicators for spike-and-slab\n",
    "        indicators = pyro.sample(\"indicators\",\n",
    "                               dist.Bernoulli(probs=spike_prob))\n",
    "        \n",
    "        # More conservative scale for local shrinkage\n",
    "        local_scale = pyro.sample(\"local_scale\", \n",
    "                                dist.HalfCauchy(torch.tensor(0.1, device=device)))\n",
    "        \n",
    "        # Combine spike and slab components with better numerical stability\n",
    "        scale = torch.clamp(\n",
    "            indicators * global_scale * local_scale + \n",
    "            (1 - indicators) * config[\"min_scale\"],\n",
    "            min=config[\"min_scale\"],\n",
    "            max=1.0\n",
    "        )\n",
    "        \n",
    "        # Use Normal distribution instead of Laplace for better stability\n",
    "        beta = pyro.sample(\"beta\", \n",
    "                          dist.Normal(torch.zeros(p, device=device), scale))\n",
    "    \n",
    "    # Compute risk scores with better numerical stability\n",
    "    risk_scores = torch.clamp(X @ beta, min=-config[\"max_risk_score\"], max=config[\"max_risk_score\"])\n",
    "    \n",
    "    # Compute likelihood\n",
    "    cox_likelihood = CoxPartialLikelihood()\n",
    "    log_pl = cox_likelihood(risk_scores, survival_time, event)\n",
    "    pyro.factor(\"log_pl\", log_pl)\n",
    "    \n",
    "    return beta\n",
    "\n",
    "def guide(X, survival_time, event):\n",
    "    n, p = X.shape\n",
    "    device = X.device\n",
    "    \n",
    "    # More conservative initialization for global scale\n",
    "    global_scale_loc = pyro.param(\n",
    "        \"global_scale_loc\",\n",
    "        torch.tensor(0.1, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    global_scale_scale = pyro.param(\n",
    "        \"global_scale_scale\",\n",
    "        torch.tensor(0.01, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    \n",
    "    # More informative initialization for spike probability\n",
    "    spike_prob_alpha = pyro.param(\n",
    "        \"spike_prob_alpha\",\n",
    "        torch.tensor(2.0, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    spike_prob_beta = pyro.param(\n",
    "        \"spike_prob_beta\",\n",
    "        torch.tensor(2.0, device=device),\n",
    "        constraint=constraints.positive\n",
    "    )\n",
    "    \n",
    "    pyro.sample(\n",
    "        \"global_scale\",\n",
    "        dist.LogNormal(global_scale_loc.log(), global_scale_scale)\n",
    "    )\n",
    "    \n",
    "    pyro.sample(\n",
    "        \"spike_prob\",\n",
    "        dist.Beta(spike_prob_alpha, spike_prob_beta)\n",
    "    )\n",
    "    \n",
    "    with pyro.plate(\"features\", p, dim=-1):\n",
    "        # More conservative initialization for indicators\n",
    "        indicator_probs = pyro.param(\n",
    "            \"indicator_probs\",\n",
    "            torch.ones(p, device=device) * 0.1,  # Start with sparse model\n",
    "            constraint=constraints.interval(0.0, 1.0)\n",
    "        )\n",
    "        \n",
    "        # More conservative initialization for local scales\n",
    "        local_scale_loc = pyro.param(\n",
    "            \"local_scale_loc\",\n",
    "            torch.ones(p, device=device) * 0.1,\n",
    "            constraint=constraints.positive\n",
    "        )\n",
    "        local_scale_scale = pyro.param(\n",
    "            \"local_scale_scale\",\n",
    "            torch.ones(p, device=device) * 0.01,\n",
    "            constraint=constraints.positive\n",
    "        )\n",
    "        \n",
    "        beta_loc = pyro.param(\n",
    "            \"beta_loc\",\n",
    "            torch.zeros(p, device=device)\n",
    "        )\n",
    "        beta_scale = pyro.param(\n",
    "            \"beta_scale\",\n",
    "            torch.ones(p, device=device) * 0.1,\n",
    "            constraint=constraints.positive\n",
    "        )\n",
    "        \n",
    "        pyro.sample(\"indicators\", dist.Bernoulli(indicator_probs))\n",
    "        \n",
    "        pyro.sample(\n",
    "            \"local_scale\",\n",
    "            dist.LogNormal(local_scale_loc.log(), local_scale_scale)\n",
    "        )\n",
    "        \n",
    "        # Use Normal distribution instead of Laplace\n",
    "        pyro.sample(\"beta\", dist.Normal(beta_loc, beta_scale))\n",
    "\n",
    "def initialize_params():\n",
    "    # This function ensures all parameters are registered before training\n",
    "    device = config[\"device\"]\n",
    "    \n",
    "    # Initialize with dummy tensors just to register parameters\n",
    "    X_dummy = torch.zeros((10, 1000), device=device)\n",
    "    time_dummy = torch.zeros(10, device=device)\n",
    "    event_dummy = torch.zeros(10, device=device)\n",
    "    \n",
    "    # Run model and guide once with dummy data to register all parameters\n",
    "    pyro.clear_param_store()\n",
    "    try:\n",
    "        model(X_dummy, time_dummy, event_dummy)\n",
    "        guide(X_dummy, time_dummy, event_dummy)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing parameters: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Print registered parameters\n",
    "    print(\"\\nInitialized parameters:\")\n",
    "    for name, param in pyro.get_param_store().items():\n",
    "        print(f\"  {name}: {param.shape}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "#=========================\n",
    "def setup_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        # Set deterministic settings using the new PyTorch 2.x syntax\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cuda.deterministic = True  # Changed from cuda_deterministic\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False  # Disable TF32 for deterministic results\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        \n",
    "        # Print GPU information\n",
    "        print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    else:\n",
    "        print(\"\\nUsing CPU\")\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    print(f\"\\nLoading data from: {path}\")\n",
    "    data = pd.read_csv(path)\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    \n",
    "    X = data.iloc[:, 1:-2].values.astype(np.float32)\n",
    "    time_values = data['time'].values.astype(np.float32)\n",
    "    event_values = data['event'].values.astype(np.float32)\n",
    "    \n",
    "    X = (X - np.nanmean(X, axis=0)) / (np.nanstd(X, axis=0) + 1e-8)\n",
    "    X = np.nan_to_num(X)\n",
    "    \n",
    "    # Move all data to GPU upfront\n",
    "    X_tensor = torch.tensor(X, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    time_tensor = torch.tensor(time_values, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    event_tensor = torch.tensor(event_values, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    \n",
    "    print(f\"Number of features: {X_tensor.shape[1]}\")\n",
    "    print(f\"Number of samples: {X_tensor.shape[0]}\")\n",
    "    print(f\"Event rate: {event_tensor.float().mean().item():.2%}\")\n",
    "    \n",
    "    return X_tensor, time_tensor, event_tensor\n",
    "\n",
    "class BayesianCoxModel:\n",
    "    def __init__(self):\n",
    "        self.device = config[\"device\"]\n",
    "        pyro.clear_param_store()\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "        X_dummy = torch.zeros((2, 1000), device=self.device, dtype=config[\"precision\"])\n",
    "        time_dummy = torch.zeros(2, device=self.device, dtype=config[\"precision\"])\n",
    "        event_dummy = torch.zeros(2, device=self.device, dtype=config[\"precision\"])\n",
    "        \n",
    "        model(X_dummy, time_dummy, event_dummy)\n",
    "        guide(X_dummy, time_dummy, event_dummy)\n",
    "\n",
    "    def train(self, train_loader, val_loader=None):\n",
    "        optimizer = ClippedAdam({\n",
    "            \"lr\": config[\"initial_lr\"],\n",
    "            \"clip_norm\": config[\"clip_norm\"],\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"betas\": (0.9, 0.999)\n",
    "        })\n",
    "        \n",
    "        svi = SVI(model, guide, optimizer, loss=Trace_ELBO(num_particles=config[\"elbo_particles\"]))\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        patience = 0\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(config[\"max_epochs\"]):\n",
    "                epoch_loss = 0.0\n",
    "                for batch in train_loader:\n",
    "                    X_batch, t_batch, e_batch = [b.to(self.device) for b in batch]\n",
    "                    \n",
    "                    # Scale the batch data\n",
    "                    X_batch = (X_batch - X_batch.mean(0)) / (X_batch.std(0) + 1e-8)\n",
    "                    \n",
    "                    loss = svi.step(X_batch, t_batch, e_batch)\n",
    "                    epoch_loss += loss\n",
    "                \n",
    "                avg_loss = epoch_loss / len(train_loader)\n",
    "                losses.append(avg_loss)\n",
    "                \n",
    "                if val_loader:\n",
    "                    val_loss = self._evaluate(val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        patience = 0\n",
    "                    else:\n",
    "                        patience += 1\n",
    "                    \n",
    "                    if patience >= config[\"early_stop_patience\"]:\n",
    "                        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                \n",
    "                if epoch % config[\"decay_step\"] == 0 and epoch > 0:\n",
    "                    optimizer.set_state({'lr': config[\"initial_lr\"] * (config[\"lr_decay\"] ** (epoch // config[\"decay_step\"]))})\n",
    "                \n",
    "                if epoch % config[\"log_freq\"] == 0:  # Use log_freq from config\n",
    "                    print(f\"Epoch {epoch:4d} | Train Loss: {avg_loss:.2f}\" + \n",
    "                          (f\" | Val Loss: {val_loss:.2f}\" if val_loader else \"\"))\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining stopped by user\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during training: {str(e)}\")\n",
    "        \n",
    "        return losses, val_losses\n",
    "\n",
    "    def _evaluate(self, loader):\n",
    "        total_loss = 0.0\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                for batch in loader:\n",
    "                    X_batch, t_batch, e_batch = [b.to(self.device) for b in batch]\n",
    "                    # Scale the batch data\n",
    "                    X_batch = (X_batch - X_batch.mean(0)) / (X_batch.std(0) + 1e-8)\n",
    "                    total_loss += pyro.infer.Trace_ELBO(num_particles=5).loss(\n",
    "                        model, guide, X_batch, t_batch, e_batch\n",
    "                    )\n",
    "            return total_loss / len(loader)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during evaluation: {str(e)}\")\n",
    "            return float('inf')\n",
    "            \n",
    "    def save_checkpoint(self, epoch, val_loss):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'val_loss': val_loss,\n",
    "            'param_store': pyro.get_param_store().get_state()\n",
    "        }\n",
    "        torch.save(checkpoint, \n",
    "                  os.path.join(self.config[\"results_dir\"], f'best_model.pth'))\n",
    "\n",
    "def cross_validate(X, time, event):\n",
    "    kf = KFold(n_splits=config[\"n_folds\"], shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    device = config[\"device\"]\n",
    "    \n",
    "    # Keep original data on GPU\n",
    "    X = X.to(device)\n",
    "    time = time.to(device)\n",
    "    event = event.to(device)\n",
    "    \n",
    "    # Create CUDA generator\n",
    "    generator = torch.Generator(device=device)\n",
    "    generator.manual_seed(42)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X.cpu().numpy())):\n",
    "        print(f\"\\nTraining fold {fold+1}/{config['n_folds']}\")\n",
    "        try:\n",
    "            # Convert indices to GPU tensors\n",
    "            train_idx = torch.tensor(train_idx, device=device)\n",
    "            test_idx = torch.tensor(test_idx, device=device)\n",
    "            \n",
    "            # Create subsets directly on GPU\n",
    "            X_train = X[train_idx]\n",
    "            time_train = time[train_idx]\n",
    "            event_train = event[train_idx]\n",
    "            \n",
    "            X_test = X[test_idx]\n",
    "            time_test = time[test_idx]\n",
    "            event_test = event[test_idx]\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset = TensorDataset(X_train, time_train, event_train)\n",
    "            test_dataset = TensorDataset(X_test, time_test, event_test)\n",
    "            \n",
    "            # Create data loaders without generator\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                shuffle=True,\n",
    "                pin_memory=False\n",
    "            )\n",
    "            \n",
    "            test_loader = DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                pin_memory=False\n",
    "            )\n",
    "            \n",
    "            # Initialize and train model\n",
    "            model_instance = BayesianCoxModel()\n",
    "            train_losses, val_losses = model_instance.train(train_loader, test_loader)\n",
    "            \n",
    "            # Compute metrics\n",
    "            with torch.no_grad():\n",
    "                beta = pyro.param(\"beta_loc\")\n",
    "                risk_scores = X_test @ beta\n",
    "                \n",
    "                c_index = concordance_index(\n",
    "                    time_test.cpu().numpy(),\n",
    "                    -risk_scores.cpu().numpy(),\n",
    "                    event_test.cpu().numpy()\n",
    "                )\n",
    "                \n",
    "                probs = pyro.param(\"indicator_probs\").cpu().numpy()\n",
    "            \n",
    "            fold_results.append({\n",
    "                'c_index': c_index,\n",
    "                'selected_features': (probs > 0.5).astype(int),\n",
    "                'train_loss': train_losses,\n",
    "                'val_loss': val_losses,\n",
    "                'last_epoch': len(train_losses)\n",
    "            })\n",
    "            \n",
    "            print(f\"Fold {fold+1} completed - C-index: {c_index:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during fold {fold+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return fold_results\n",
    "    \n",
    "def plot_diagnostics(results_dir, fold_results, losses, val_losses_list, param_trajectories):\n",
    "    plots_dir = os.path.join(results_dir, 'plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(losses, label='Training Loss', color='blue', linewidth=2)\n",
    "    \n",
    "    if val_losses_list:\n",
    "        for fold_idx, val_losses in enumerate(val_losses_list):\n",
    "            plt.plot(val_losses, label=f'Validation Loss (Fold {fold_idx+1})', \n",
    "                     linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title('Training and Validation Loss Convergence', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('ELBO Loss', fontsize=12)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'loss_convergence.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot feature selection probabilities\n",
    "    if 'indicator_probs' in param_trajectories:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        final_probs = param_trajectories['indicator_probs'][-1]\n",
    "        plt.hist(final_probs, bins=50, alpha=0.7)\n",
    "        plt.title('Feature Selection Probabilities Distribution', fontsize=14)\n",
    "        plt.xlabel('Selection Probability', fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, 'feature_selection_dist.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot selected features heatmap\n",
    "    selected_features = np.stack([result['selected_features'] for result in fold_results])\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(selected_features, cmap='coolwarm', center=0.5,\n",
    "                xticklabels=False, yticklabels=range(1, len(fold_results) + 1))\n",
    "    plt.title('Selected Features Across Folds', fontsize=14)\n",
    "    plt.xlabel('Feature Index', fontsize=12)\n",
    "    plt.ylabel('Fold', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'selected_features.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot C-index distribution\n",
    "    c_indices = [result['c_index'] for result in fold_results]\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.violinplot(c_indices, showmeans=True, showmedians=True)\n",
    "    plt.plot([1] * len(c_indices), c_indices, 'o', color='blue', alpha=0.7)\n",
    "    mean_c = np.mean(c_indices)\n",
    "    std_c = np.std(c_indices)\n",
    "    plt.text(1.2, min(c_indices), \n",
    "             f'Mean: {mean_c:.4f}\\nStd: {std_c:.4f}', \n",
    "             fontsize=12, bbox=dict(facecolor='white', alpha=0.7))\n",
    "    plt.title('Cross-validation C-index Distribution', fontsize=14)\n",
    "    plt.ylabel('C-index', fontsize=12)\n",
    "    plt.xticks([1], ['C-index'])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, 'cv_c_index.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    setup_gpu()\n",
    "    os.makedirs(config[\"results_dir\"], exist_ok=True)\n",
    "    \n",
    "    # Save configuration\n",
    "    with open(os.path.join(config[\"results_dir\"], 'config.json'), 'w') as f:\n",
    "        json.dump({k: str(v) if isinstance(v, (torch.dtype, type)) else v \n",
    "                  for k, v in config.items()}, f, indent=2)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Current Date and Time (UTC - YYYY-MM-DD HH:MM:SS formatted): {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Current User's Login: {os.getlogin()}\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        X_tensor, time_tensor, event_tensor = load_and_preprocess(config[\"data_path\"])\n",
    "        \n",
    "        # Run cross-validation\n",
    "        fold_results = cross_validate(X_tensor, time_tensor, event_tensor)\n",
    "        \n",
    "        # Process cross-validation results\n",
    "        all_val_losses = [result['val_losses'] for result in fold_results if 'val_losses' in result]\n",
    "        cv_stopping_epochs = [result['last_epoch'] for result in fold_results if 'last_epoch' in result]\n",
    "        \n",
    "        optimal_epochs = int(np.mean(cv_stopping_epochs)) if cv_stopping_epochs else config[\"max_epochs\"]\n",
    "        print(f\"\\nUsing {optimal_epochs} epochs for final model based on cross-validation\")\n",
    "        \n",
    "        # Update config for final model\n",
    "        final_config = config.copy()\n",
    "        final_config[\"max_epochs\"] = optimal_epochs\n",
    "        \n",
    "        # Create dataset for final training\n",
    "        dataset = TensorDataset(X_tensor, time_tensor, event_tensor)\n",
    "        train_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            pin_memory=False  # Removed generator\n",
    "        )\n",
    "        \n",
    "        # Train final model\n",
    "        print(f\"\\nTraining final model on full dataset for {optimal_epochs} epochs\")\n",
    "        final_model = BayesianCoxModel()\n",
    "        losses, _ = final_model.train(train_loader)\n",
    "        \n",
    "        # Save results\n",
    "        results_summary = {\n",
    "            'mean_c_index': float(np.mean([r['c_index'] for r in fold_results])),\n",
    "            'std_c_index': float(np.std([r['c_index'] for r in fold_results])),\n",
    "            'final_loss': float(losses[-1]) if losses else float('nan'),\n",
    "            'n_epochs_final': len(losses),\n",
    "            'optimal_epochs': optimal_epochs,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(config[\"results_dir\"], 'results_summary.json'), 'w') as f:\n",
    "            json.dump(results_summary, f, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\nTraining completed\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main execution: {e}\")\n",
    "        torch.cuda.empty_cache()\n",
    "    finally:\n",
    "        print(\"\\nExecution finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7daecca-c450-4e27-b815-da0490bd6d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de728424-54ea-4b3b-ba14-d17fa3707f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd28fdf-2b29-4d44-ad1c-658a3078578f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f6482-14c4-4925-9d0e-8119d398f793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa08406-8b75-4bbb-a605-a6bc66255031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CHIKOM~1\\ANACON~1\\envs\\vb_cox_pymc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "\n",
      "Loading data from: D:\\cox-model-imputation\\error-in-r-code-for-mcar\\datasets\\vb-cox\\realistic_cox_data.csv\n",
      "Data shape: (800, 1003)\n",
      "Number of features: 1000\n",
      "Number of samples: 800\n",
      "Event rate: 58.88%\n",
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch    0 | Train Loss: 7879570247.65 | Val Loss: 7898330759.41\n",
      "Epoch   50 | Train Loss: 7828337776.54 | Val Loss: 7885955143.18\n",
      "Epoch  100 | Train Loss: 7840899969.42 | Val Loss: 7853007554.40\n",
      "\n",
      "Early stopping at epoch 113\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch    0 | Train Loss: 7921804795.10 | Val Loss: 7920152238.15\n",
      "Epoch   50 | Train Loss: 7819492229.12 | Val Loss: 7891488643.13\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import ClippedAdam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from torch.distributions import constraints  # Changed this line\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    # Training parameters\n",
    "    \"max_epochs\": 800,\n",
    "    \"batch_size\": 64,\n",
    "    \"initial_lr\": 1e-5,\n",
    "    \"lr_decay\": 0.1,\n",
    "    \"decay_step\": 200,\n",
    "    \"clip_norm\": 1.0,\n",
    "    \"early_stop_patience\": 50,\n",
    "    \n",
    "    # Device\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"precision\": torch.float32,\n",
    "    \n",
    "    # Model parameters\n",
    "    \"elbo_particles\": 5,\n",
    "    \"n_folds\": 5,\n",
    "    \n",
    "    # Regularization parameters\n",
    "    \"max_risk_score\": 5.0,\n",
    "    \"min_scale\": 1e-6,\n",
    "    \"init_spike_prob\": 0.01,\n",
    "    \"global_scale_prior\": 0.01,\n",
    "    \"lasso_scale\": 0.01,\n",
    "    \n",
    "    # Paths\n",
    "    \"data_path\": r\"D:\\cox-model-imputation\\error-in-r-code-for-mcar\\datasets\\vb-cox\\realistic_cox_data.csv\",\n",
    "    \"results_dir\": f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "}\n",
    "\n",
    "class CoxPartialLikelihood(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-8\n",
    "        self.max_exp = 5.0\n",
    "        \n",
    "    def forward(self, risk_scores, survival_time, event):\n",
    "        event_mask = event == 1\n",
    "        n_events = event_mask.sum()\n",
    "        \n",
    "        if n_events < 1:\n",
    "            return torch.tensor(0.0, device=risk_scores.device)\n",
    "        \n",
    "        # Normalize risk scores\n",
    "        risk_scores = (risk_scores - risk_scores.mean()) / (risk_scores.std() + self.eps)\n",
    "        risk_scores = torch.clamp(risk_scores, -self.max_exp, self.max_exp)\n",
    "        \n",
    "        sorted_times, indices = torch.sort(survival_time, descending=True)\n",
    "        risk_scores = risk_scores[indices]\n",
    "        event_mask = event_mask[indices]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            max_rs = risk_scores.max()\n",
    "        \n",
    "        risk_exp = torch.exp(risk_scores - max_rs)\n",
    "        cumsum_risk = torch.cumsum(risk_exp, 0)\n",
    "        log_cumsum = torch.log(cumsum_risk + self.eps)\n",
    "\n",
    "        pl = risk_scores[event_mask] - log_cumsum[event_mask]\n",
    "        return -pl.mean()  # Negative log-likelihood\n",
    "\n",
    "def model(X, survival_time, event):\n",
    "    n, p = X.shape\n",
    "    device = X.device\n",
    "    \n",
    "    # Move scalar tensors to device\n",
    "    global_scale = pyro.sample(\"global_scale\", \n",
    "                             dist.HalfNormal(torch.tensor(config[\"global_scale_prior\"], device=device)))\n",
    "    spike_prob = pyro.sample(\"spike_prob\", \n",
    "                           dist.Beta(torch.tensor(1.05, device=device), \n",
    "                                   torch.tensor(10.0, device=device)))\n",
    "    \n",
    "    with pyro.plate(\"features\", p):\n",
    "        indicators = pyro.sample(\"indicators\", dist.Bernoulli(spike_prob))\n",
    "        local_scale = pyro.sample(\"local_scale\", \n",
    "                                dist.HalfNormal(torch.tensor(0.05, device=device)))\n",
    "        \n",
    "        scale = torch.clamp(\n",
    "            indicators * global_scale * local_scale + (1 - indicators) * config[\"min_scale\"],\n",
    "            min=config[\"min_scale\"],\n",
    "            max=1.0\n",
    "        )\n",
    "        \n",
    "        beta = pyro.sample(\"beta\", \n",
    "                          dist.Laplace(torch.zeros(p, device=device), \n",
    "                                     scale * config[\"lasso_scale\"]))\n",
    "    \n",
    "    risk_scores = torch.clamp(X @ beta, -config[\"max_risk_score\"], config[\"max_risk_score\"])\n",
    "    log_pl = CoxPartialLikelihood()(risk_scores, survival_time, event)\n",
    "    pyro.factor(\"log_pl\", log_pl)\n",
    "\n",
    "def guide(X, survival_time, event):\n",
    "    n, p = X.shape\n",
    "    device = X.device\n",
    "    \n",
    "    # Move all parameters to device\n",
    "    global_scale_loc = pyro.param(\"global_scale_loc\", \n",
    "                                torch.tensor(0.01, device=device),\n",
    "                                constraint=constraints.positive)\n",
    "    global_scale_scale = pyro.param(\"global_scale_scale\",\n",
    "                                  torch.tensor(0.001, device=device),\n",
    "                                  constraint=constraints.positive)\n",
    "    \n",
    "    global_scale = pyro.sample(\"global_scale\",\n",
    "                             dist.LogNormal(global_scale_loc, global_scale_scale))\n",
    "    \n",
    "    spike_alpha = pyro.param(\"spike_prob_alpha\",\n",
    "                           torch.tensor(1.05, device=device),\n",
    "                           constraint=constraints.positive)\n",
    "    spike_beta = pyro.param(\"spike_prob_beta\",\n",
    "                          torch.tensor(10.0, device=device),\n",
    "                          constraint=constraints.positive)\n",
    "    \n",
    "    spike_prob = pyro.sample(\"spike_prob\",\n",
    "                           dist.Beta(spike_alpha, spike_beta))\n",
    "    \n",
    "    with pyro.plate(\"features\", p):\n",
    "        indicator_probs = pyro.param(\n",
    "            \"indicator_probs\",\n",
    "            torch.full((p,), config[\"init_spike_prob\"], device=device),\n",
    "            constraint=constraints.interval(0.0, 1.0)\n",
    "        )\n",
    "        \n",
    "        local_scale_loc = pyro.param(\"local_scale_loc\",\n",
    "                                   torch.full((p,), 0.05, device=device),\n",
    "                                   constraint=constraints.positive)\n",
    "        local_scale_scale = pyro.param(\"local_scale_scale\",\n",
    "                                     torch.full((p,), 0.01, device=device),\n",
    "                                     constraint=constraints.positive)\n",
    "        \n",
    "        beta_loc = pyro.param(\"beta_loc\",\n",
    "                            torch.zeros(p, device=device))\n",
    "        beta_scale = pyro.param(\"beta_scale\",\n",
    "                              torch.full((p,), 0.1, device=device),\n",
    "                              constraint=constraints.positive)\n",
    "        \n",
    "        pyro.sample(\"indicators\", dist.Bernoulli(indicator_probs))\n",
    "        pyro.sample(\"local_scale\", dist.LogNormal(local_scale_loc, local_scale_scale))\n",
    "        pyro.sample(\"beta\", dist.Normal(beta_loc, beta_scale))\n",
    "\n",
    "def setup_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"\\nUsing CPU\")\n",
    "\n",
    "def load_and_preprocess(path):\n",
    "    print(f\"\\nLoading data from: {path}\")\n",
    "    data = pd.read_csv(path)\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    \n",
    "    X = data.iloc[:, 1:-2].values.astype(np.float32)\n",
    "    time_values = data['time'].values.astype(np.float32)\n",
    "    event_values = data['event'].values.astype(np.float32)\n",
    "    \n",
    "    X = (X - np.nanmean(X, axis=0)) / (np.nanstd(X, axis=0) + 1e-8)\n",
    "    X = np.nan_to_num(X)\n",
    "    \n",
    "    # Move all data to GPU upfront\n",
    "    X_tensor = torch.tensor(X, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    time_tensor = torch.tensor(time_values, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    event_tensor = torch.tensor(event_values, dtype=config[\"precision\"]).to(config[\"device\"])\n",
    "    \n",
    "    print(f\"Number of features: {X_tensor.shape[1]}\")\n",
    "    print(f\"Number of samples: {X_tensor.shape[0]}\")\n",
    "    print(f\"Event rate: {event_tensor.float().mean().item():.2%}\")\n",
    "    \n",
    "    return X_tensor, time_tensor, event_tensor\n",
    "\n",
    "class BayesianCoxModel:\n",
    "    def __init__(self):\n",
    "        self.device = config[\"device\"]\n",
    "        pyro.clear_param_store()\n",
    "        self._initialize_params()\n",
    "\n",
    "    def _initialize_params(self):\n",
    "        X_dummy = torch.zeros((2, 1000), device=self.device, dtype=config[\"precision\"])\n",
    "        time_dummy = torch.zeros(2, device=self.device, dtype=config[\"precision\"])\n",
    "        event_dummy = torch.zeros(2, device=self.device, dtype=config[\"precision\"])\n",
    "        \n",
    "        model(X_dummy, time_dummy, event_dummy)\n",
    "        guide(X_dummy, time_dummy, event_dummy)\n",
    "\n",
    "    def train(self, train_loader, val_loader=None):\n",
    "        optimizer = ClippedAdam({\n",
    "            \"lr\": config[\"initial_lr\"],\n",
    "            \"clip_norm\": config[\"clip_norm\"],\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"betas\": (0.9, 0.999),  # Add momentum parameters\n",
    "        })\n",
    "        \n",
    "        svi = SVI(model, guide, optimizer, loss=Trace_ELBO(num_particles=config[\"elbo_particles\"]))\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        patience = 0\n",
    "        \n",
    "        try:\n",
    "            for epoch in range(config[\"max_epochs\"]):\n",
    "                epoch_loss = 0.0\n",
    "                for batch in train_loader:\n",
    "                    X_batch, t_batch, e_batch = [b.to(self.device) for b in batch]\n",
    "                    \n",
    "                    # Scale the batch data\n",
    "                    X_batch = (X_batch - X_batch.mean(0)) / (X_batch.std(0) + 1e-8)\n",
    "                    \n",
    "                    loss = svi.step(X_batch, t_batch, e_batch)\n",
    "                    epoch_loss += loss\n",
    "                \n",
    "                avg_loss = epoch_loss / len(train_loader)\n",
    "                losses.append(avg_loss)\n",
    "                \n",
    "                if val_loader:\n",
    "                    val_loss = self._evaluate(val_loader)\n",
    "                    val_losses.append(val_loss)\n",
    "                    \n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        patience = 0\n",
    "                    else:\n",
    "                        patience += 1\n",
    "                    \n",
    "                    if patience >= config[\"early_stop_patience\"]:\n",
    "                        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                \n",
    "                if epoch % config[\"decay_step\"] == 0 and epoch > 0:\n",
    "                    optimizer.set_state({'lr': config[\"initial_lr\"] * (config[\"lr_decay\"] ** (epoch // config[\"decay_step\"]))})\n",
    "                \n",
    "                if epoch % 50 == 0:\n",
    "                    print(f\"Epoch {epoch:4d} | Train Loss: {avg_loss:.2f}\" + \n",
    "                          (f\" | Val Loss: {val_loss:.2f}\" if val_loader else \"\"))\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining stopped by user\")\n",
    "        \n",
    "        return losses, val_losses\n",
    "\n",
    "    def _evaluate(self, loader):\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                X_batch, t_batch, e_batch = [b.to(self.device) for b in batch]\n",
    "                total_loss += pyro.infer.Trace_ELBO(num_particles=5).loss(\n",
    "                    model, guide, X_batch, t_batch, e_batch\n",
    "                )\n",
    "        return total_loss / len(loader)\n",
    "\n",
    "def cross_validate(X, time, event):\n",
    "    kf = KFold(n_splits=config[\"n_folds\"], shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    device = config[\"device\"]\n",
    "    \n",
    "    # Keep original data on GPU\n",
    "    X = X.to(device)\n",
    "    time = time.to(device)\n",
    "    event = event.to(device)\n",
    "    \n",
    "    # Generate splits\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X.cpu().numpy())):\n",
    "        print(f\"\\n=== Fold {fold+1}/{config['n_folds']} ===\")\n",
    "        \n",
    "        # Convert indices to GPU tensors\n",
    "        train_idx = torch.tensor(train_idx, device=device)\n",
    "        test_idx = torch.tensor(test_idx, device=device)\n",
    "        \n",
    "        # Create subsets directly on GPU\n",
    "        X_train = X[train_idx]\n",
    "        time_train = time[train_idx]\n",
    "        event_train = event[train_idx]\n",
    "        \n",
    "        X_test = X[test_idx]\n",
    "        time_test = time[test_idx]\n",
    "        event_test = event[test_idx]\n",
    "        \n",
    "        # Create datasets (data already on GPU)\n",
    "        train_dataset = TensorDataset(X_train, time_train, event_train)\n",
    "        test_dataset = TensorDataset(X_test, time_test, event_test)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=config[\"batch_size\"]\n",
    "        )\n",
    "        \n",
    "        # Initialize and train model\n",
    "        model = BayesianCoxModel()\n",
    "        train_losses, val_losses = model.train(train_loader, test_loader)\n",
    "        \n",
    "        # Compute metrics\n",
    "        with torch.no_grad():\n",
    "            beta = pyro.param(\"beta_loc\")  # Already on GPU\n",
    "            risk_scores = X_test @ beta\n",
    "            \n",
    "            # Move to CPU only for concordance index calculation\n",
    "            c_index = concordance_index(\n",
    "                time_test.cpu().numpy(),\n",
    "                -risk_scores.cpu().numpy(),\n",
    "                event_test.cpu().numpy()\n",
    "            )\n",
    "            \n",
    "            probs = pyro.param(\"indicator_probs\").cpu().numpy()\n",
    "        \n",
    "        fold_results.append({\n",
    "            'c_index': c_index,\n",
    "            'selected_features': (probs > 0.5).astype(int),\n",
    "            'train_loss': train_losses,\n",
    "            'val_loss': val_losses\n",
    "        })\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "def save_results(results_dir, fold_results):\n",
    "    with open(os.path.join(results_dir, 'results.json'), 'w') as f:\n",
    "        json.dump({\n",
    "            'c_indices': [res['c_index'] for res in fold_results],\n",
    "            'mean_c_index': float(np.mean([res['c_index'] for res in fold_results])),\n",
    "            'std_c_index': float(np.std([res['c_index'] for res in fold_results])),\n",
    "            'selected_features': [res['selected_features'].tolist() for res in fold_results]\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    with open(os.path.join(results_dir, 'config.json'), 'w') as f:\n",
    "        safe_config = {k: str(v) if isinstance(v, torch.dtype) else v for k, v in config.items()}\n",
    "        json.dump(safe_config, f, indent=2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, res in enumerate(fold_results):\n",
    "        plt.plot(res['train_loss'], label=f'Fold {i+1} Train')\n",
    "        plt.plot(res['val_loss'], '--', label=f'Fold {i+1} Val')\n",
    "    plt.title(\"Training/Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.boxplot([res['c_index'] for res in fold_results])\n",
    "    plt.title(\"Cross-Validation C-Index\")\n",
    "    plt.ylabel(\"C-Index\")\n",
    "    plt.savefig(os.path.join(results_dir, 'c_indices.png'))\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    setup_gpu()\n",
    "    os.makedirs(config[\"results_dir\"], exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        X, time, event = load_and_preprocess(config[\"data_path\"])\n",
    "        fold_results = cross_validate(X, time, event)\n",
    "        save_results(config[\"results_dir\"], fold_results)\n",
    "        \n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        print(f\"Results saved to: {config['results_dir']}\")\n",
    "        print(f\"Average C-index: {np.mean([res['c_index'] for res in fold_results]):.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18108d56-3de9-438a-8a0a-28d66adcc7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
